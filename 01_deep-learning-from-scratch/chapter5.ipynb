{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydTsfITlhbn1"
   },
   "source": [
    "# 第5章 誤差伝播法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bjMl5PnefwK6"
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive/work/\n",
    "# !git clone https://github.com/oreilly-japan/deep-learning-from-scratch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15124,
     "status": "ok",
     "timestamp": 1655019871173,
     "user": {
      "displayName": "Yuki Katayama",
      "userId": "15528639115256770261"
     },
     "user_tz": -540
    },
    "id": "IREB4Wc_gSVm",
    "outputId": "1ec6b67f-81f8-47f9-89f4-352e0281a64d"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "# %cd /content/drive/MyDrive/work/deep-learning-from-scratch/ch04/\n",
    "# %cd /deep-learning-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1655026909736,
     "user": {
      "displayName": "Yuki Katayama",
      "userId": "15528639115256770261"
     },
     "user_tz": -540
    },
    "id": "mG5wuXNxSnc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jovyan/work/dl_study/01_deep-learning-from-scratch', '/opt/conda/lib/python38.zip', '/opt/conda/lib/python3.8', '/opt/conda/lib/python3.8/lib-dynload', '', '/opt/conda/lib/python3.8/site-packages', '/opt/conda/lib/python3.8/site-packages/IPython/extensions', '/home/jovyan/.ipython', '/home/jovyan/work/dl_study/01_deep-learning-from-scratch/refs']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# importディレクトリの追加\n",
    "# sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "sys.path.append(os.path.join(Path().resolve(), 'refs'))\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "print(urllib.request.getproxies())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 計算グラフ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T.B.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 連鎖率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T.B.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 逆伝播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T.B.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 単純なレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 乗算レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- レイヤは`foward()`と`backward()`の共通メソッドを持つようにする\n",
    "- 乗算レイヤはMulLayerという名前にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y  # 順伝播ではxとyを乗算するだけ\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        # パラメータXにおける偏微分(dL/dx)\n",
    "        dx = dout * self.y    # 逆伝播時はxとyをひっくり返す\n",
    "        # パラメータXにおける偏微分(dL/dy)        \n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 乗算レイヤの使用例\n",
    "\n",
    "以下の図5-16の実装を行う\n",
    "\n",
    "![fig5-16](./images/fig5-16.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 220.00000000000003\n",
      "dapple_price: 1.1, dtax: 200\n",
      "dapple: 2.2, dapple_num: 110.00000000000001\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# foward(順伝播)\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)    # 1層目\n",
    "price = mul_tax_layer.forward(apple_price, tax)            # 2層目\n",
    "\n",
    "print(f'price: {price}')\n",
    "\n",
    "# backward(逆伝播)\n",
    "dprice = 1    # 最終金額の逆伝播は定数なので1\n",
    "\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)          # 2層目\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # 1層目\n",
    "\n",
    "print(f'dapple_price: {dapple_price}, dtax: {dtax}')\n",
    "print(f'dapple: {dapple}, dapple_num: {dapple_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2. 加算レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加算レイヤは初期化が必要ない\n",
    "- `foward()`では, 加算レイヤでは2つの引数`x, y`を受け取り、それを加算して出力する\n",
    "- `backward()`では, 蒸留から伝わってきた微分(dout)をそのまま下流に流す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加算レイヤと乗算レイヤの使用例\n",
    "\n",
    "- 以下の図5-17を実装する\n",
    "\n",
    "![fig5-17](./images/fig5-17.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 715.0000000000001\n",
      "dapple_num: 715.0000000000001, dapple: 2.2\n",
      "dorange_num: 165.0, dorange: 3.3000000000000003\n",
      "dtax: 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# foward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)               # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)           # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)                         # (4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)                         # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)           # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)               # (1)\n",
    "\n",
    "print(f'price: {price}')  # -> 715\n",
    "print(f'dapple_num: {price}, dapple: {dapple}')           # => 2.2, 110\n",
    "print(f'dorange_num: {dorange_num}, dorange: {dorange}')  # => 3.3, 165\n",
    "print(f'dtax: {dtax}')    # => 650 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 活性化関数レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 ReLUレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU(Rectified Linear Unit)は次の式で表せた (5.7)\n",
    "\n",
    "$$\n",
    "    ReLU(y) =\n",
    "        \\begin{cases}\n",
    "            x \\quad (x > 0) \\\\\n",
    "            0 \\quad (x \\leqq 0) \\\\\n",
    "        \\end{cases}\n",
    "$$\n",
    "\n",
    "ReLU(5.7)のxに関するyの微分は以下のように求められる(5.8)\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial y}{\\partial x} =\n",
    "        \\begin{cases}\n",
    "            1 \\quad (x > 0) \\\\\n",
    "            0 \\quad (x \\leqq 0) \\\\\n",
    "        \\end{cases}\n",
    "$$\n",
    "\n",
    "![fig5-18](./images/fig5-18.PNG)\n",
    "\n",
    "\n",
    "- **順伝播時の入力xが0より大きければ, 逆伝播は上流の値をそのまま下流に流す \\[左図\\]** \n",
    "\n",
    "- **順伝播時の入力xが0以下のときは, 逆伝播では何も流さない(0を流す) \\[右図\\]** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "\n",
    "x = np.array( [[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mask変数はTrue/FalseからなるNumpy配列\n",
    "- 順伝播の入力であるxの要素で0以下の場所をTrue, それ以外をFalseとして保持する\n",
    "- 逆伝播では順伝播時に保持したmaskを使い, 上流からdoutにおける0以下の要素を0に設定する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.2 Sigmoidレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **「exp」ノード**は, $ y = \\exp(x) $の計算を行う\n",
    "- **「／」ノード**は, $ y = \\frac{1}{x} $の計算を行う\n",
    "\n",
    "![fig5-20](./images/fig5-20.PNG)\n",
    "\n",
    "### Step1. \n",
    "- $ y = \\frac{1}{x} $の微分は解析的に次の式で表される\n",
    "$$\n",
    "    \\frac{\\partial y}{\\partial x} = -\\frac{1}{x^2} = -y^2\n",
    "$$\n",
    "\n",
    "- 逆伝播のときは, 上流の値に対して$ -y^2 $を乗算して下流に伝播する\n",
    "  - (順伝播の出力の2乗にマイナスをつけた値)\n",
    "  \n",
    "### Step2.\n",
    "- \"+\"ノードは上流の値を下流にそのまま流すだけ\n",
    "\n",
    "### Step3.\n",
    "\n",
    "- \"exp\"ノードの微分は以下で表される\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial y}{\\partial x} = \\exp(x)\n",
    "$$\n",
    "- sigmoid関数は微分しても, sigmoid関数となる\n",
    "\n",
    "### Step4. \n",
    "- \"×\"ノードは, 順伝播時の値をひっくり返して乗算する\n",
    "\n",
    "---\n",
    "\n",
    "- 図5-20の計算グラフの逆伝播出力は $ \\frac {\\partial L}{\\partial y} y^2 \\exp(-x) $となる\n",
    "\n",
    "- これはさらに整理して書くと以下のようになる\n",
    "\n",
    "$$\n",
    "\n",
    "    \\frac{\\partial L}{\\partial y} y^2 \\exp(-x) = \\frac{\\partial L}{\\partial y}\\frac{1}{(1 + \\exp(-x)^2)^2}\\exp(-x) \\\\    \n",
    "    \\frac{\\partial L}{\\partial y} y(1 - y)\n",
    "$$\n",
    "\n",
    "![fig5-22](./images/fig5-22.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **sigmoidレイヤは順伝播時に出力を保持しておいて、逆伝播時の計算に利用するのがポイント**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        # 順伝播時に出力をoutに保持しておく\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # 逆伝播時に過去の出力結果を用いる\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine／Softmaxレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nnの順伝播では重み付き信号の総和計算に **行列の積**`np.dot()`を利用した\n",
    "\n",
    "- ここでXを入力, Wを重み, Bをバイアスとし、それぞれ(2,), (2,3), (3)の多次元配列であるとする\n",
    "\n",
    "- そうするとNNの重み付き和は`Y = np.dot(X,W) + B`のように計算できる\n",
    "\n",
    "- 行列の積の計算は対応する次元の要素数を一致させるのがポイント\n",
    "\n",
    "- NNの順伝播で行う行列の積は幾何学分野ではアフィン変換と呼ばれる\n",
    "    - アフィン変換を行う処理をAffineレイヤという名前で実装する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 行列を対象とした逆伝播を求める場合, 行列の要素後に書き下すことでスカラを対象とした計算グラフと同じ手順で考えることができる\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial \\boldsymbol{Y}} \\cdot W^T \\\\\n",
    "    \\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial \\boldsymbol{Y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig5-25](./images/fig5-25.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 バッチ版Affineレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N個のデータをまとめて入力可能なAffineレイヤを考える\n",
    "\n",
    "- 入力であるXの形状が(N, 2)になっただけ\n",
    "\n",
    "- 前と同じように計算グラフ上で, 素直に行列の計算をするだけ\n",
    "\n",
    "- 逆伝播の際は, 行列の形状に注意すれば(?), スカラの場合と同様に計算できる\n",
    "\n",
    "- バイアスの加算は注意が必要\n",
    "\n",
    "  - 順伝播のバイアス加算は X・Wに対して, バイアスがそれぞれ加算される\n",
    "  \n",
    "  - 逆伝播の際には, **それぞれのデータの逆伝播の値がバイアスの要素に集約される必要がある**(?)\n",
    "  \n",
    "  - バイアスの逆伝播はその2個のデータに対しての微分を, **データ毎に合算して求める**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])\n",
    "\n",
    "print(X_dot_W)\n",
    "\n",
    "print(X_dot_W + B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(dY)\n",
    "\n",
    "dB= np.sum(dY, axis=0)\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.w) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        \n",
    "        # np.sum()で0番目の軸に対しての総和を求め, バッチ単位で合算する\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Lossレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Softmaxレイヤは, 入力された値を正規化(総和が1になるように変形)する\n",
    "\n",
    "- 損失関数であるクロスエントロピー誤差も含めた\"Softmax-with-Loss\"レイヤを実装する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig5-28](./images/fig5-28.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig5-29](./images/fig5-29.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ここでは、3 クラス分類を行う場合を想定し、前レイヤから3 つの入力（スコア）を受け取るものとする\n",
    "\n",
    "- Softmaxレイヤは, 入力である$ (a_1, a_2, a_3) $を正規化して, $ (y_1, y_2, y_3) $を出力する\n",
    "\n",
    "- Cross Entropy Errorレイヤは, Softmaxの出力$ (y_1, y_2, y_3) $と教師ラベル$ (t_1, t_2, t_3) $を受け取り, 損失$ L $を出力する\n",
    "\n",
    "- Softmaxレイヤからの逆伝播は, $ (y_1-t_1, y_2-t_2 , y_-t_3 ) $という**Softmaxレイヤの出力と教師ラベルの差分になる**\n",
    "\n",
    "\n",
    "## 具体例\n",
    "\n",
    "- 教師ラベルが$ (0, 1, 0) $であるデータに対して、Softmaxレイヤの出力が$ (0.3, 0.2, 0.5) $ であった場合を考える。\n",
    "\n",
    "  - 正解ラベルに対する確率は20%なので, この時点ではNNの学習が不足している\n",
    "  \n",
    "  - この場合, Softmaxレイヤからの逆伝播は$ (0.3, -0.8, 0.5) $という**大きな誤差が伝播される**\n",
    "  \n",
    "  - その結果, Softmaxよりも前のレイヤは, **<u>その大きな誤差から大きな内容を学習することになる</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.loss = None  # 損失\n",
    "        self.y = None     # Softmaxの出力\n",
    "        self.t = None     # 教師データ(one-hot表現)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "        \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 誤差逆伝播法の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- レイヤを組み合わせることで、レゴブロックを組み合わせるようにNNを構築することができる\n",
    "- 数値微分は簡単に実装できる反面, 計算に時間がかかる\n",
    "- 誤差逆伝播法は計算時間を早くでき, 効率よく勾配を求めることができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.1 NNの学習の全体図\n",
    "\n",
    "- ニューラルネットワークは、適応可能な**重み**と**バイアス**があり、<u>この重みとバイアスを訓練データに適応するように調整することを「学習」と呼ぶ</u>\n",
    "- ニューラルネットワークの学習は次の4 つの手順で行う。\n",
    "\n",
    "1. **(ミニバッチ)** 訓練データの中からランダムに一部のデータを選び出す\n",
    "2. **(勾配の算出)** 各重みパラメータに関する損失関数の勾配を求める\n",
    "3. **(パラメータの更新)** 重みパラメータを勾配方向に微小量だけ更新する。\n",
    "4. **(繰り返す)** ステップ1～3を繰り返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        \n",
    "        self.params = {}\n",
    "        # 重みは正規分布に基づく分散0.01の乱数で初期化する\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        # バイアスはゼロで初期化する\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        \n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        '''データとラベルを入力し、そのlossを返す\n",
    "        args\n",
    "          x: 入力データ\n",
    "          t: 教師データ\n",
    "        '''\n",
    "        \n",
    "        # 最終層(SoftmaxwithLoss)直前までの順伝播する\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        # 最終層に直前のレイヤの出力を入力し, Lossを返す\n",
    "        return self.lastLayer.forward(y, t)\n",
    "        \n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        \n",
    "        # 入力された教師データが複数ある場合は, 最も大きいものを返す(One-Hot表現対応?)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        # ラベルが一致する正解データの割合(精度)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        '''各層の偏微分を行い, 勾配を求める\n",
    "        x: 入力データ\n",
    "        t: 教師データ        \n",
    "        '''\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_w, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_w, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_w, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_w, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = 1    # 最終層の微分結果は1(定数項しかないので)\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        # OrderedDictをreverseで逆順にすると, 後ろからの逆伝播を簡単に記述できる\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()        \n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7.3 誤差逆伝播法の勾配確認\n",
    "\n",
    "- 数値微分は<u>実装が簡単であるため、ミスが起きにくい</u>\n",
    "- 誤差逆伝播法は<u>実装が複雑になるため, ミスが起きやすい</u>\n",
    "\n",
    "- 数値微分の結果と誤差逆伝播法の結果を比較し, 誤差逆伝播法の実装の正しさを確認することがよく行われる**(勾配確認)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: 3.7143381633662783e-10\n",
      "b1: 2.2330886705462104e-09\n",
      "W2: 6.112464970068129e-09\n",
      "b2: 1.4005502981112584e-07\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from ch05.two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 各重みの絶対誤差の平均を求める\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(f'{key}: {str(diff)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7.4 誤差逆伝播法を使った学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from ch05.two_layer_net import TwoLayerNet\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "        \n",
    "    # 誤差逆伝播法によって勾配を求める\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 重みとバイアスを更新する\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        # 算出した勾配に LRを乗算し, 次の重みパラメータに設定する\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f'it[{i:4d}], train_acc: {train_acc}, test_acc: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 まとめ\n",
    "\n",
    "- 計算グラフを用いてNNの誤差逆伝播法を説明\n",
    "- ReLuレイヤやSoftmax-with-Lossレイヤ, Affineレイヤ, Softmaxレイヤ等を学んだ\n",
    "\n",
    "- forward(), backwardというメソッドを実装することで\n",
    "\n",
    "### 本章で学んだこと\n",
    "\n",
    "- 計算グラフを用いれば, 計算過程を視覚的に把握することができる\n",
    "- 計算グラフのノードは局所的な計算によって構成される. 局所的な計算が全体の計算を構成する\n",
    "- 計算グラフの順伝播は, 通常の計算を行う. \n",
    "- 計算グラフの逆伝播では, 各ノードの微分を求まる\n",
    "- NNの構成要素をレイヤとして実装し, 勾配計算を効率的に行える(**誤差逆伝播法**)\n",
    "- 数値微分と誤差逆伝播法の結果を比較すると, 誤差逆伝播法の実装に誤りがないことを確認できる(**勾配確認**)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNXXc7E9tbg+hzbnUcVyZ73",
   "collapsed_sections": [],
   "mount_file_id": "1WogcERx00fk9B9Xl3aBVUJkFkrwx_VE2",
   "name": "chpater4-2_4-3_4-4.ipynb",
   "provenance": [
    {
     "file_id": "1hTlLv27iqJTeVxKw9t1tovSZ6oJN7C0v",
     "timestamp": 1655019431203
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
