{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydTsfITlhbn1"
   },
   "source": [
    "# 4.1 データから学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bjMl5PnefwK6"
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive/work/\n",
    "# !git clone https://github.com/oreilly-japan/deep-learning-from-scratch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IREB4Wc_gSVm",
    "outputId": "dce23dda-ad73-4782-b6da-b92390db80e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './deep-learning-from-scratch/ch04/'\n",
      "/home/jovyan/work/DL_scratch\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/drive/MyDrive/work/deep-learning-from-scratch/ch04/\n",
    "%cd ./deep-learning-from-scratch/ch04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stUQ12LhnhLM"
   },
   "source": [
    "- NNの特徴はデータから学習できる点にある\n",
    "- **学習**とは<u>重みパラメータの値をデータから自動で決定すること</u>をいう\n",
    "- パーセプトロンによる実装は線形分離可能な問題であれば解くことができる\n",
    "    - 有限回の学習によって、線形分離可能な問題がとけることはことは**パーセプトロンの収束定理**として知られている\n",
    "    - パーセプトロンでは非線形分離問題は解くことができない\n",
    "\n",
    "- **画像から特徴を抽出**し、その特徴量のパターンを機械学習の技術で学習する\n",
    "- 特徴量とは、入力データから本質的なデータを的確に抽出できるように設計された変換器\n",
    "- 画像の特徴量は通常は**ベクトルとして表現される**\n",
    "    - 特徴量にはSIFT, SURF, HOG等が挙げられる\n",
    "- 特徴量を使って画像データをベクトルに変換し、変換されたベクトルに対して、機械学習の識別器で学習させることができるできる\n",
    "    - 識別器とははSVMややKNNを指す\n",
    "\n",
    "- このアプローチは集められたデータの中から機械が規則性を見つけることである\n",
    "- これはゼロからアルゴリズムを考えるよりも、効率的に問題を解決でき、人の負担を減らすことにつながる\n",
    "\n",
    "- ただし画像をベクトルに変換する部分は人が設計したものを使うことに注意が必要\n",
    "    - これは、問題に適した特徴量を設計しなければ、良い結果が得られないということ\n",
    "    - つまり、特徴量と機械学習によるアプローチでも**問題に応じて人の手によって適した特徴量を考える必要がある**\n",
    "\n",
    "- 対して、NNによるアプローチでは画像を\"そのまま\"学習する\n",
    "\n",
    "- NNでは、画像に含まれる重要な特徴量を人が設計する必要はない\n",
    "\n",
    "- NNの利点は, 問題毎に独自に設計する必要なく、**生データから目的の結果が得られる(end-to-end)点にある**\n",
    "\n",
    "- NNは与えられたデータをひたすら学習して、与えられたデ問題のパターンを発見しようとする\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyOpOwUAt3Sn"
   },
   "source": [
    "# 4.1.2 訓練データとテストデータ\n",
    "\n",
    "- 機械学習の問題ではでは, データを訓練データとテストデータの2つに分けて学習と実験を行うのが一般的\n",
    "\n",
    "- まず訓練データだけを使って学習を行い, 最適なパラメータを探す\n",
    "- その後, テストデータを使って訓練したモデルの実力を評価する\n",
    "\n",
    "- 汎化能力を正しく評価したいがために、訓練データとテストデータを分離する\n",
    "    - 訓練データは教師データとも呼ばれる\n",
    "- 汎化能力とは、まだ見ぬデータに対しての判断能力のこと\n",
    "- ひとつのデータセットだけでパラメータの学習と評価を行ってしまうと正しい評価が行えないことになる\n",
    "- あるデータセットだけに過度に対応した状態を**過学習(overfitting)**と呼ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KVjC-cpvXzR"
   },
   "source": [
    "## 4.2. 損失関数\n",
    "\n",
    "- NNの学習で用いられる指標は損失関数(loss function)と呼ばれる\n",
    "- 損失関数は任意の関数を用いることができるが、一般には**二乗和誤差**や**交差エントロピー誤差**を用いる\n",
    "\n",
    "- 損失関数はNNの性能の**\"悪さ\"**を表す指標\n",
    "    - 現在のNNが教師データに対して、<u>どれだけ適合していないのか</u>ということを表す\n",
    "    - 損失関数にマイナスをかけた値は、<u>\"どれだけ性能が悪くないか\"(=どれだけ性能が良いか)</u>を表す指標として解釈できる\n",
    "    - \"性能の悪さを最小にする\"ことと、\"性能の良さを最大にする\"ことは、**本質的に同じ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 二乗和誤差\n",
    "\n",
    "- 最も有名な損失関数\n",
    "- 定義\n",
    "    - ここで$y_{k}$はNNの出力, $t_{k}$は教師データを表し、$k$はデータの次元数を表す\n",
    "$$\n",
    "    E = \\frac{1}{2}\\sum(y_{k}-t_{k})^{2}\n",
    "$$\n",
    "\n",
    "- 二乗和誤差は**NNの出力と, 正解となる教師データの各要素の差を二乗してその総和を求める**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- yはSoftmax関数の出力\n",
    "- Softmax関数の出力は確率として解釈できるので、下記例では0の確立は0.1, 1の確立は0.05, 2の確立は0.6と表すことができる\n",
    "- tは教師データを表す, この場合はt[2]が1なので、正解は2であることを表す(One-hot表現)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squared_error(y, t):\n",
    "    print(f'y.argmax: {y.argmax()}')\n",
    "    # NNの出力と, 正解となる教師データの各要素の差を二乗してその総和を求める\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.argmax: 2\n",
      "0.09750000000000003\n",
      "y.argmax: 7\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "# 「2」を正解とする\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# 例1：「2」の確率が最も高い場合（0.6）\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(f'{sum_squared_error(np.array(y), np.array(t))}')\n",
    "\n",
    "# 例2：「7」の確率が最も高い場合（0.6）\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(f'{sum_squared_error(np.array(y), np.array(t))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 例1と例2では、例1の方が損失関数の出力値が小さくなっている \n",
    "    - 例1: 正解を\"2\"として, NNの出力結果も\"2\"が最も高い場合**(推論結果が正しいケース)**\n",
    "    - 例2: 正解を\"2\"として, NNの出力結果は\"6\"が最も高い場合**(推論結果が誤っているケース)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. 交差エントロピー誤差\n",
    "\n",
    "- 定義: \n",
    "$$\n",
    "    E = -\\sum_{k}(t_{k} - \\log{y_{k}})\n",
    "$$\n",
    "\n",
    "- ここで$\\log$は自然対数($log_{e}$)を表し, $y_{k}$はNNの出力, $t_{k}$は正解ラベルとする\n",
    "- $t_{k}$はラベルとなるインデックスだけが1で, そのほかは0とする(One-hot表現)\n",
    "- 交差エントロピー誤差では正解ラベルに対応する出力の自然対数を計算するだけ\n",
    "- 交差エントロピー誤差は、**正解ラベルとなる出力の結果$y_{k}?$によってその値が決まる**。\n",
    "\n",
    "- 自然対数の性質として, x=1のときはy=0であり, xが0に近づくほどyの値は小さくなる\n",
    "- **正解ラベルに対応するNNの推論結果$y_{k}$が大きいほど, 交差エントロピー誤差の出力は0に近づき, 出力値が1.0のとき, 交差エントロピー誤差は0になる**\n",
    "- つまり**推論時の確率が高いほど, 交差エントロピー誤差の出力値は小さくなる**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    print(f'debug: {t*np.log(y + delta)}')\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(f'{cross_entropy_error(np.array(y), np.array(t))}')\n",
    "\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(f'{cross_entropy_error(np.array(y), np.array(t))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3. ミニバッチ学習\n",
    "\n",
    "- 訓練データを使って学習することは以下のようなこと\n",
    "    - 訓練データに対する損失関数を求め\n",
    "    - その値をできるだけ小さくするようなパラメータを探し出す\n",
    "    \n",
    "- 損失関数はすべての訓練データを対象として求める必要がある\n",
    "- 訓練データが100個あれば, その100個の損失関数の和を指標とする\n",
    "\n",
    "- 訓練データすべての損失関数の和を求めたいとすると, 交差エントロピー誤差の場合は以下のようになる\n",
    "\n",
    "$$\n",
    "    E = -\\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}\\log{y_{nk}}\n",
    "$$\n",
    "\n",
    "- ここで, $N$はデータの総数, $t_{nk}$は$n$個目のデータの$k$番目の値を意味する\n",
    "- これは, **全ての交差エントロピー誤差の総和を取り, データの個数${N}$で割って正規化したものとなる**\n",
    "    - $N$で割ることによって, ひとつあたりの平均の損失関数を求めることになる\n",
    "    - 平均化すれば訓練データの数に影響されず、いつでも統一した指標が得られる\n",
    "\n",
    "- すべてのデータを対象にして損失関数の和を求めるのは現実的ではない\n",
    "    - データ件数が多いと、総和を求めるのに時間がかかるため\n",
    "\n",
    "- そこで, ミニバッチ学習では**データの中から一部を選び出し, その一部のデータを全体の近似として利用する**\n",
    "    - ミニバッチは小さな塊という意味\n",
    "    - 例: MNISTでは訓練データ総数60000枚のうち, 100枚を無作為に選び出して, 学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (60000, 784)\n",
      "test shape: (60000, 10)\n",
      "batch_mask: [50770 12312 46080  8713 12013 30478  5840 47956  1534 43240]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from reference.dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "# 訓練データ形状(データ個数, ピクセル数)\n",
    "print(f'train shape: {x_train.shape}')\n",
    "\n",
    "# 教師データ形状(データ個数, ラベル数(one-hot enc))\n",
    "print(f'test shape: {t_train.shape}')\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "\n",
    "# np.random.choice()を使用して、\n",
    "# 指定された数字の中からランダムに好きな数だけを取り出す\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "print(f'batch_mask: {batch_mask}')\n",
    "\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 ミニバッチ対応版 交差エントロピー誤差の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        y: NNの出力値(Softmaxの出力値)\n",
    "        t: 教師データ\n",
    "    \"\"\"\n",
    "    \n",
    "    if y.ndim == 1:\n",
    "        # データが一つしかない場合(yの次元数が1の場合)はデータの形状を整形する\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    # バッチ枚数を取得して正規化し、1枚あたりの平均交差エントロピー誤差を計算する\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "    # 教師データがラベルとして与えられたとき(tがintのとき)は以下のように実装できる\n",
    "    # return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.log( y[np.arange(batch_size), t] )の実装解説\n",
    "\n",
    "- np.arange(batch_size) は、0 からbatch_size-1 までの配列を生成します。\n",
    "   - たとえば、batch_size が5 だとしたら、np.arange(batch_size)は[0, 1, 2, 3, 4] のNumPy 配列を生成します。\n",
    "   - t にはラベルが[2, 7, 0, 9, 4] のように格納されているので、  \n",
    "     y[np.arange(batch_size), t]は、各データの正解ラベルに対応するニューラルネットワークの出力を抽出します\n",
    "    （ この例では、y[np.arange(batch_size), t] は、[y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]] のNumPy 配列を生成します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 なぜ損失関数を設定するのか？\n",
    "\n",
    "- 損失関数は何故導入されているのだろうか?\n",
    "    - 認識精度が高くなるようなパラメータを獲得したいだけなのに、わざわざ数値がする意味ってなに？\n",
    "\n",
    "- NN学習では最適なパラメータを探索する際に、損失関数の値ができるだけ小さくなるようなパラメータを探す\n",
    "- できるだけ小さな損失関数の場所を探すために、**パラメータの微分(=勾配)を計算し、その微分の値を手掛かりにパラメータを更新していく**\n",
    "\n",
    "- ひとつの重みパラメータの損失関数に対する微分は, **\"重みパラメータの値をわずかに変化させたとき, 損失関数がどのように変化するか\"**を表す\n",
    "    - **微分した値がマイナス**であれば , <u>重みパラメータを**正の方向**に変化させることで損失関数を減少させることができる</u>\n",
    "    - **微分した値がプラス**であれば, <u>重みパラメータを**負の方向**へ変化させることで損失関数を減少させることができる</u>\n",
    "\n",
    "\n",
    "- NN学習の際に認識精度を指標にしてはならない. その理由はパラメータの微分がほとんどの場所で0になってしまうから\n",
    "- 微分の値が0になると重みパラメータを更新できないので、更新はそこで止まってしまう\n",
    "\n",
    "#### 具体例: \n",
    "- あるニューラルネットワークが現在100 枚ある訓練データの中で32 枚を正しく認識できている状況を考える.\n",
    "- もし認識精度を指標にしたとすれば、重みパラメータの値を少し変えただけでは認識精度は32% のままで、変化が現れないだろう\n",
    "    - つまり, **わずかなパラメータの調整だけでは認識精度は改善されない**\n",
    "    - また, もし認識精度が改善されたとしても, その値は連続的な変化($32.0123...\\%)$ではなく, 不連続な値(33%や34%などの離散値)を取るようになる\n",
    "\n",
    "- 一方, 損失関数を指標とした場合は, **現在の損失関数の値は$0.92543...$のように, 連続値で表すことができる**\n",
    "- パラメータの値をわずかに変化させると, それに反応して損失関数も$0.93432...$のように**連続的に変化する**\n",
    "\n",
    "- 認識精度はパラメータの微小な変化にはほとんど反応しない, もし反応があってもその値は不連続にいきなり変化する\n",
    "    - 活性化関数のステップ関数にも同じ話が当てはまる(NNの学習にステップ関数が使えないのはこれが理由)    \n",
    "- ステップ関数の微分はほとんどの場所で0になり, 損失関数を指標に用いたとしても**パラメータの微小な変化を見つけることができない**\n",
    "\n",
    "- シグモイド関数の微分は出力が連続的に変化し, 曲線の傾きも連続的に変化する\n",
    "    - つまり, **シグモイド関数の微分はどの場所であっても0にならない**\n",
    "\n",
    "\n",
    "- <u>**NNの学習では, 傾きが0にならないことが非常に重要**</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chpater4-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
