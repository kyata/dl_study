{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95740630-610e-46b3-be9a-b5a7bdebe804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/y-katayama/notebooks/dl_study/02_pytorch_tutorial', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/home/y-katayama/venv/pt1.7/lib/python3.8/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# importディレクトリの追加\n",
    "# sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "print(sys.path)\n",
    "\n",
    "# プロキシの設定\n",
    "# os.environ['HTTP_PROXY'] = ''\n",
    "# os.environ['HTTPS_PROXY'] = ''\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a270e4-640b-476f-8085-751eb44a2288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 14 17:33:33 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| 40%   32C    P8    17W / 184W |   1076MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:03:00.0 Off |                  N/A |\n",
      "| 40%   29C    P8    14W / 184W |      8MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1986      G   /usr/lib/xorg/Xorg                 45MiB |\n",
      "|    0   N/A  N/A     40940      C   ...ma/venv/pt1.7/bin/python3     1026MiB |\n",
      "|    1   N/A  N/A      1986      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b08cdb-52b9-4d3b-b1aa-9eb7aab15a1d",
   "metadata": {},
   "source": [
    "# Autograd(自動微分)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495451d8-063b-4cf3-8389-41b06efcccd0",
   "metadata": {},
   "source": [
    "- autogradを使用する場合, **計算グラフでネットワークの順伝播の経路を定義する**\n",
    "- あとは省略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801980de-ce79-4f47-a30c-3c9f0aef05a9",
   "metadata": {},
   "source": [
    "## Pytorch: 新しいautograd関数の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b1d91-8fb4-41f3-882d-ee3bddf35da6",
   "metadata": {},
   "source": [
    "- autograd演算子はTensor上で動作する2つの関数を備える\n",
    "    - `forward()`: 順伝播処理. 入力Tensorからの出力Tensorを計算\n",
    "    - `backward()`: 逆伝播処理あるスカラー値の出力Tensorの勾配を受け取り, 同じスカラー値に対応する入力Tensorの勾配を計算する\n",
    "\n",
    "- `torch.autograd.Function`を継承したサブクラスを定義して, foward, backward()をオーバーライドすることで簡単に独自のautograd演算子を定義することができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f76b24bd-ad0f-43fd-9f0a-b0f45985defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"torch.autograd.Functionをサブクラス化して, \n",
    "    forward()とbackward()を定義して, 独自のautograd Functionを実装する\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"順伝播経路では入力Tensorを受け取り, 出力Tensorを返す\n",
    "        ctxは逆伝播の際に必要な情報を格納するコンテキストオブジェクト\n",
    "        ctx.save_for_backward()メソッドを使用すると, \n",
    "        渡されたデータを保持して逆伝播時に使用できる\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        逆伝播経路では, 出力に対する損失の勾配を含むTensorを受け取り\n",
    "        入力に対する損失の勾配を計算する必要がある\n",
    "        \"\"\"\n",
    "        \n",
    "        # 順伝播時に保持していたTensorを呼び出す\n",
    "        input, = ctx.saved_tensors\n",
    "        \n",
    "        # 下位レイヤから渡された勾配tensorを複製\n",
    "        grad_input = grad_output.clone()\n",
    "        \n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "324ecf8b-d82b-44f8-ba97-01d2a2544f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 388.609375\n",
      "199 1.6020762920379639\n",
      "299 0.01111598126590252\n",
      "399 0.00026563520077615976\n",
      "499 4.242756403982639e-05\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# N：バッチサイズ         D_in：入力層の次元数\n",
    "# H：隠れ層の次元数       D_out： 出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 乱数により入力データと目標となる出力データを表すTensorを生成\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 乱数による重みを表すTensorの定義\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 関数を適用するには、Function.applyメソッドを用います。\n",
    "    # reluと命名しておきます。\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # 順伝播：独自のautograd操作を用いてReLUの出力を算出することで予想結果yを計算します。\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # 損失の計算と表示\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autogradを利用して逆伝播を実施\n",
    "    loss.backward()\n",
    "\n",
    "    # 確率的勾配降下法を用いた重みの更新\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 重みの更新後、手動で勾配を0に初期化\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9975e-f827-4b07-8e36-1feb63b7c779",
   "metadata": {},
   "source": [
    "# nnモジュール"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f2b84-0c49-4d0b-8c0e-22217eb54bd7",
   "metadata": {},
   "source": [
    "- nnパッケージはニューラルネットワークのレイヤーとほぼ同等なモジュールのセットを定義している\n",
    "- 他にも損失関数等の便利関数群も含まれる\n",
    "- torch.nnはTensorflowでのKeras, TensorFlow-Slim, TFLearnに相当するパッケージ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdec690b-73d3-446f-98bb-3bdf93ca1dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.690643072128296\n",
      "199 0.04399355873465538\n",
      "299 0.0015361031983047724\n",
      "399 7.08393199602142e-05\n",
      "499 4.083602561877342e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N：バッチサイズ         D_in：入力層の次元数\n",
    "# H：隠れ層の次元数       D_out： 出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nnパッケージを利用して, レイヤーの連なりとしてモデルを定義\n",
    "# nn.Sequentialは他のモジュールとして並べて定義することで、各レイヤを順番に実行して出力を得る\n",
    "# nn.Linearは入力から出力を線形変換する. 重みとバイアスを保持する\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out)\n",
    ")\n",
    "\n",
    "# 損失関数はMSEを使用する\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    \n",
    "    # 順伝播処理\n",
    "    # Pythonのモジュールオブジェクトを継承しているため__call__が呼びだされる\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # 損失の計算と表示\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # 逆伝播の前に勾配を0フィル\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # 逆伝播\n",
    "    # 内部ではrequires_grad=TrueとなっているすべてのTensorにモデルのパラメータが保持されている\n",
    "    # モデルが持つ学習可能なパラメータの勾配を計算できる\n",
    "    loss.backward()\n",
    "    \n",
    "    # SGDを用いた重みの更新\n",
    "    # 各々のパラメータはTensorなので, これまでと同じ方法で勾配を参照することができる\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b62b9d7a-9b9c-42a6-9d34-3b052d305fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 41.6004638671875\n",
      "199 0.4085378646850586\n",
      "299 0.005604689475148916\n",
      "399 9.623980440665036e-05\n",
      "499 8.11555935342767e-07\n"
     ]
    }
   ],
   "source": [
    "# N：バッチサイズ         D_in：入力層の次元数\n",
    "# H：隠れ層の次元数       D_out： 出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 乱数により入力データと目標となる出力データを表すTensorを生成\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nnパッケージを用いてモデルと損失関数を定義\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# optimパッケージでモデルの重みを更新するオプティマイザを定義する\n",
    "# optimパッケージには多くの最適化アルゴリズムが存在するが, ここではAdamを使う\n",
    "# Adamのコンストラクタの最初の引数により、オプティマイザがどのTensorを更新するか指定できる。\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # 順伝播:入力xから予測値yをモデルで算出します。。\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 損失の計算と表示\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 逆伝播に入る前に、更新されることになる変数（モデルの学習可能な重み）の勾配を\n",
    "    # optimaizerを使用して0に初期化します。\n",
    "    # これは、デフォルトで.backward()が呼び出される度に勾配がバッファに蓄積されるため\n",
    "    # 必要になる操作です（上書きされるわけではない）。\n",
    "    # 詳しくはtorch.autograd.backwardのドキュメントを参照してください。\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 逆伝播：モデルのパラメータに対応する損失の勾配を計算\n",
    "    loss.backward()\n",
    "\n",
    "    # オプティマイザのstep関数を呼び出すことでパラメータを更新\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957ea25-5249-40a5-83d5-5e29f60f2e7c",
   "metadata": {},
   "source": [
    "# nn Modulesの改変"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d16a29-77f3-4e10-8ccb-1f66acaaf454",
   "metadata": {},
   "source": [
    "- 既存のモジュールにある一連の演算処理を改変したい場合, `nn.Module`をサブクラス化し, \n",
    "  他のモジュールやTensorの他のautograd操作を利用して, 入力Tensorや出力Tensorを生成するforward()メソッドを定義することで, ユーザ独自のモジュールを作ることができる\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1a82822-a3ff-4393-b69c-acb3694240b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.353790521621704\n",
      "199 0.04485118389129639\n",
      "299 0.0015129691455513239\n",
      "399 6.243730604182929e-05\n",
      "499 2.8527128961286508e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        コンストラクタにより2つのnn.Linearモジュールをインスタンスとし定義し\n",
    "        それらをメンバ変数に割り当てます。\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward関数は入力データのTensorを受け入れ、出力データのTensorを返します。\n",
    "        Tensorの任意の演算子と同様に、コンストラクタで定義されたモジュールを使用できます。\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N：バッチサイズ         D_in：入力層の次元数\n",
    "# H：隠れ層の次元数       D_out： 出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 乱数により入力データと目標となる出力データを表すTensorを生成\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 上で定義したクラスをインスタンス化してモデルを構築\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# 損失関数とオプティマイザを定義します。\n",
    "# model.parameters()を呼び出すことで、モデルのメンバ変数である2つのnnn.Linearモジュールの\n",
    "# 学習可能なパラメータをSGDのコンストラクタの引数として渡すことができます。\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # 順伝播:入力xから予測値yをモデルで算出します。\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 損失の計算と表示\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 勾配を0に初期化し、逆伝播を実行することで重みを更新\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3903d586-a392-4203-ab6a-b701edebcfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 83.9114761352539\n",
      "199 12.065067291259766\n",
      "299 10.240217208862305\n",
      "399 0.3152943253517151\n",
      "499 0.5636540055274963\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        コンストラクタにより、順伝播で使用する3層のnn.Linearのインスタンスを定義します。\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        順伝播の経路の実装では、ランダムに0から3までの値を選択し、\n",
    "        その数だけ中間層のモジュールを再利用することで、隠れ層の出力を計算します。\n",
    "\n",
    "        それぞれの順伝播の経路を表す計算グラフは動的に変化するので、\n",
    "        繰り返しや、条件分岐といったPythonの標準的なフロー制御を利用して\n",
    "        順伝播の経路を定義することができます。\n",
    "\n",
    "        この結果から確認できるように、計算グラフを定義する際に、問題なく何度も同じ\n",
    "        モジュールを使いまわすことができるのは、一度しかモジュールを利用することが\n",
    "        できなったLua Torchから大きく改善したところと言えます。\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 4)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N：バッチサイズ         D_in：入力層の次元数\n",
    "# H：隠れ層の次元数       D_out： 出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 乱数により入力データと目標となる出力データを表すTensorを生成\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 上で定義したクラスをインスタンス化してモデルを構築します。\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# 損失関数とオプティマイザを定義します。\n",
    "# この奇妙なモデルを通常の確率勾配降下法で訓練するのは難しいので、モーメンタムを使用します。\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "    # 順伝播:入力xから予測値yをモデルで算出します。\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 損失の計算と表示\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 勾配を0に初期化し、逆伝播を実行することで重みを更新します\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459898f-da13-4d57-9198-c28d6a404d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt1.7",
   "language": "python",
   "name": "pt1.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
